{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b0f183-93f7-4ee2-90cf-05660660ec56",
   "metadata": {},
   "source": [
    "# Gradio App answering questions about a provided pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5b26e-b13d-41f0-956d-8d80f21a0b7d",
   "metadata": {},
   "source": [
    "Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b297f9-3ad3-4ae0-b6a9-2a49804e577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import chromadb\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54e6f2-5da4-44ab-9817-c53c331370ab",
   "metadata": {},
   "source": [
    "The model is a GGUF quantized version of Mistral's Zephyr 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86089932-4668-4333-bf14-995ca8dcc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/zephyr-7B-beta-GGUF\", \n",
    "    model_file=\"zephyr-7b-beta.Q4_K_M.gguf\", \n",
    "    model_type=\"mistral\", hf=True\n",
    ")\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained(\n",
    "    'HuggingFaceH4/zephyr-7b-beta', use_fast=True\n",
    ")\n",
    "\n",
    "pipe=pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c835ab-0c8a-422e-b45c-10e5b195c724",
   "metadata": {},
   "source": [
    "## Auxiliary functions which process the document and feed the relevant information into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16d056ac-c7a5-4b6c-a324-2d150c58aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt template for inference. We pass the question which is fed into the template for the tokenizer. The output is the prompt for the model\n",
    "def Prompt(question: str)->str:\n",
    "    #Template to be used for the tokenizer\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot who always responds in a polite manner\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": ''},\n",
    "    ]\n",
    "    #We add the input question to the template\n",
    "    messages[1]['content']=question\n",
    "    query = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return query\n",
    "\n",
    "# We load a pdf file with PyPDFLoader. The function returns a list of documents (one per page)\n",
    "def document_loader(file)->list:\n",
    "    loader=PyPDFLoader(file)\n",
    "    loaded_document=loader.load_and_split()\n",
    "    return loaded_document \n",
    "\n",
    "#We split the loaded pdf into chunks of text. Returns a list of documents (one for each chunk)\n",
    "def text_splitter(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=350,\n",
    "        chunk_overlap=20,\n",
    "        separators=[\"\\n\\n\", \"\"], #this instead of the default separators to improve chunk overlaps\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "#Returns the 3 chunks of text most relevant for the query, as determined by chromadb\n",
    "def doc_database(chunks: list[str],query: str)->list[str]:\n",
    "    \n",
    "    #chunk_list retains for each chunk only the text (page_content) and removes metadata. To be used for the Chroma collection\n",
    "    chunk_list=list(map(lambda x: x.page_content, chunks))  \n",
    "    \n",
    "    #List of ids for the Chroma collection (list of strings)\n",
    "    ids=list(map(str,range(len(chunks))))\n",
    "    \n",
    "    client=chromadb.Client()\n",
    "    collection=client.get_or_create_collection(\n",
    "        name='doc_collection',\n",
    "        configuration={\n",
    "            \"hnsw\": {\n",
    "                \"space\": \"cosine\" #The default metric is L^2 but cosine is more suited for text similarity\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    collection.upsert(ids=ids, documents=chunk_list)\n",
    "    results= collection.query(query_texts=[query], n_results=3)  \n",
    "    return results['documents'][0] #results['documents'] is a list with a sinle element, which is the list of relevant chunks\n",
    "\n",
    "def text_question(query: str, database: list[str])->str:\n",
    "    '''\n",
    "    inputs:\n",
    "           query (the question we ask to the chatbot)\n",
    "           database (the list of relevant text chunks from doc_database)\n",
    "    outputs the answer of the chatbot\n",
    "    '''\n",
    "    #We append to the prompt template the text contained in database\n",
    "    text=''\n",
    "    for i in range(len(database)):\n",
    "        text += database[i]\n",
    "        \n",
    "    template='Considering the following text, can you explain'+query+'text:'+text   \n",
    "    limit=510-len(tokenizer.encode(Prompt(template)))   #set this as a limit for the generated tokens to fit in the context window (512 tokens)\n",
    "    output=pipe(Prompt(template),max_new_tokens=limit, temperature=0.2, do_sample=True)\n",
    "    question_answer=output[0]['generated_text'].split('<|assistant|>') #We select only the text generated by the chatbot\n",
    "    return question_answer[1]\n",
    "\n",
    "#Function which combines all the previous ones to generate a chatbot answer about the pdf file from a query\n",
    "def doc_qa(query: str, file)->str:\n",
    "    '''\n",
    "    Example: query='your question' file='filename'\n",
    "    print(doc_qa(query, file))\n",
    "    '''\n",
    "    data = document_loader(file)\n",
    "    chunks = text_splitter(data)\n",
    "    database = doc_database(chunks,query)\n",
    "    return text_question(query, database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1041e4-3175-4510-aa6a-e785dc195b17",
   "metadata": {},
   "source": [
    "## Gradio User Interface architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479f35b-9e9b-4210-9204-3201b32e1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=doc_qa,\n",
    "    allow_flagging='never',\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\"),\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\")  # Drag and drop file upload\n",
    "    ],\n",
    "    outputs=gr.Textbox(label='Output'),\n",
    "    title='QA Chatbot',\n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
