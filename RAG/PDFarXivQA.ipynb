{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a5bac6-48f8-437f-90a7-64df9b76b860",
   "metadata": {},
   "source": [
    "# Gradio App answering questions about a pdf file or a file uploaded from the arXiv website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582bf448-e44a-45d0-9165-8bec3586776a",
   "metadata": {},
   "source": [
    "Necessary libraries and script to download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716da13-801d-420c-8d54-5331f4b81553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter \n",
    "from langchain_community.document_loaders import PyPDFLoader, ArxivLoader\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "\n",
    "token = os.getenv('token_hf')  #HuggingFace access token\n",
    "login(token=token)\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "\tfilename=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    n_ctx = 4096,  # Context window length\n",
    "    n_threads = 8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=0, # The number of layers to offload to GPU, set to 0 if no GPU acceleration is available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50cee0-8457-486f-96f7-4e2149516f6a",
   "metadata": {},
   "source": [
    "## Prompt Template for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0e0632-2286-40b7-b59d-f4f1ff78e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which returns the answer of the model to a query\n",
    "def llm_output(query: str)->str:\n",
    "    output = llm(\n",
    "      f\"<|user|>\\n{query}<|end|>\\n<|assistant|>\", #format for the prompt\n",
    "      max_tokens=350,  #maximum number of tokens generated by the model\n",
    "      stop=[\"<|end|>\"], \n",
    "      echo=False,  # Whether to echo the prompt\n",
    "      temperature=0.3\n",
    "    )\n",
    "    return output['choices'][0]['text'] #output is a dictionary. We just retain the text of the provided answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741aca75-3cf5-44f1-b0bb-b0938206a5f5",
   "metadata": {},
   "source": [
    "## Auxiliary functions to handle RAG for a pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607729f-f991-4fa6-a681-248a33a31430",
   "metadata": {},
   "source": [
    "The pdf file is split into chunks which are then stored in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af79459e-1f9b-4adf-9194-59a9d4b270dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load a pdf file with PyPDFLoader. The function returns a list of documents (one per page)\n",
    "def document_loader(file)->list[dict[str]]:\n",
    "    loader=PyPDFLoader(file)\n",
    "    loaded_document=loader.load_and_split()\n",
    "    return loaded_document \n",
    "\n",
    "#We split the loaded pdf into chunks of text. Returns a list of documents (one for each chunk)\n",
    "def text_splitter(data: list[dict[str]])->list[dict[str]]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=40,\n",
    "        separators=[\"\\n\\n\", \"\"], #this instead of the default separators to improve chunk overlaps\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "#Returns the 3 chunks of text most relevant for the query, as determined by chromadb\n",
    "def doc_database(chunks: list[dict[str]], query: str)->list[str]:\n",
    "    \n",
    "    #chunk_list retains for each chunk only the text (page_content) and removes metadata. To be used for the Chroma collection\n",
    "    chunk_list=list(map(lambda x: x.page_content, chunks))  \n",
    "    \n",
    "    #List of ids for the Chroma collection (list of strings)\n",
    "    ids=list(map(str,range(len(chunks))))\n",
    "    \n",
    "    client=chromadb.Client()\n",
    "    collection=client.get_or_create_collection(\n",
    "        name='doc_collection',\n",
    "        configuration={\n",
    "            \"hnsw\": {\n",
    "                \"space\": \"cosine\" #The default metric is L^2 but cosine is more suited for text similarity\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    collection.upsert(ids=ids, documents=chunk_list)\n",
    "    results= collection.query(query_texts=[query], n_results=5)  \n",
    "    return results['documents'][0] #results['documents'] is a list with a sinle element, which is the list of relevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb616e-393e-4c16-969d-6d42fac94cf9",
   "metadata": {},
   "source": [
    "## Auxiliary functions to handle RAG for a document uploaded from the arXiv website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2320ac-6ab2-4b4a-98d8-729071b1243e",
   "metadata": {},
   "source": [
    "The document is uploaded from the arXiv website and split into chunks stored in the vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2277cf-3a9d-406d-8ee6-4baba0c2e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the document_loader function defined previously. We load a file from the arXiv website by providing the arxiv number (xxxx.xxxxx) as input.\n",
    "def arxiv_loader(arxiv_num: str)->list[dict[str]]:\n",
    "    loader=ArxivLoader(arxiv_num)\n",
    "    loaded_document=loader.load()\n",
    "    return loaded_document \n",
    "\n",
    "#Variant of the text_splitter function defined previously to handle arxiv documents\n",
    "def arxiv_text_splitter(data: list[dict[str]])-> list[str]:\n",
    "    text=data[0].page_content  #The output of arxiv_loader is a list with one element. We extract the content which is a string\n",
    "    text_splitter = CharacterTextSplitter( #since text is of type str, without separators, we can use CharacterTextSplitter\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=40,\n",
    "        separator='', \n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "#Variant of the doc_database function tailored for the arxiv documents. Returns the 5 most relevant chunks of text.\n",
    "def arxiv_database(chunks: list[str], query: str)->list[str]:\n",
    "    \n",
    "    #We do not need to preprocess chunks since in this case the output of arxiv_text_splitter is already a list of strings\n",
    "    #List of ids for the Chroma collection (list of strings)\n",
    "    ids=list(map(str,range(len(chunks))))\n",
    "    \n",
    "    client=chromadb.Client()\n",
    "    collection=client.get_or_create_collection(\n",
    "        name='arxiv_collection',\n",
    "        configuration={\n",
    "            \"hnsw\": {\n",
    "                \"space\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    collection.upsert(ids=ids, documents=chunks)\n",
    "    results= collection.query(query_texts=[query], n_results=5)  \n",
    "    return results['documents'][0] #results['documents'] is a list with a sinle element, which is the list of relevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e9ef5-2783-4bae-aa6b-e740b6007e79",
   "metadata": {},
   "source": [
    "## Answer generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71b4b2-ef85-4c4a-b678-c9eb3c4fd496",
   "metadata": {},
   "source": [
    "The retrieved chunks of text are added to the template and provide the context which allow the model to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fd49934-2df6-44b9-81dd-d8131e8f56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which generates the model output. Works for both PDF and arXiv documents\n",
    "\n",
    "def text_question(query: str, database: list[str])->str:\n",
    "    '''\n",
    "    inputs:\n",
    "           query (the question we ask to the chatbot)\n",
    "           database (the list of relevant text chunks from doc_database)\n",
    "    outputs the answer of the chatbot\n",
    "    '''\n",
    "    #We append to the prompt template the text contained in database\n",
    "    text=''\n",
    "    for i in range(len(database)):\n",
    "        text += database[i]\n",
    "        \n",
    "    template='Considering the following text, can you explain'+query+'text:'+text   \n",
    "    output=llm_output(template)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d9372-d404-4018-bf3d-1917390b9a97",
   "metadata": {},
   "source": [
    "## User Interface with gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bcda5-1e3a-442f-8fca-2bc99b95c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradio application\n",
    "\n",
    "#Function which combines all the previous ones to generate a chatbot answer about the pdf file from a query\n",
    "def pdf_qa(query: str, file)->str:\n",
    "    data = document_loader(file)\n",
    "    chunks = text_splitter(data)\n",
    "    database = doc_database(chunks,query)\n",
    "    return text_question(query, database)\n",
    "\n",
    "#Function which combines all the previous ones to generate a chatbot answer about the arxiv file from a query\n",
    "def arxiv_qa(query: str, arxiv_num: str)->str:\n",
    "    data = arxiv_loader(arxiv_num)\n",
    "    chunks = arxiv_text_splitter(data)\n",
    "    database = arxiv_database(chunks,query)\n",
    "    return text_question(query, database)\n",
    "\n",
    "\n",
    "#Code for gradio App. We include two buttons b1 and b2 which determine whether we have to process the uploaded pdf or the arXiv file respectively.\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Document QA\n",
    "    Upload a PDF File and press 'File upload' or provide the arXiv number and press 'File from arXiv'. The bot will answer your question.\n",
    "    \"\"\")\n",
    "    query  = gr.Textbox(label=\"Input Question\", lines=2, placeholder=\"Type your question here...\")\n",
    "    file   = gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\")  # Drag and drop file upload\n",
    "    text   = gr.Textbox(label=\"arXiv number\", lines=2, placeholder=\"Enter a valid arXiv number\")\n",
    "    answer = gr.Textbox(label=\"Output\")\n",
    "\n",
    "    b1 = gr.Button(\"File upload\")\n",
    "    b2 = gr.Button(\"File from arXiv\")\n",
    "\n",
    "    b1.click(pdf_qa, inputs=[query, file], outputs=answer)\n",
    "    b2.click(arxiv_qa, inputs=[query, text], outputs=answer)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
